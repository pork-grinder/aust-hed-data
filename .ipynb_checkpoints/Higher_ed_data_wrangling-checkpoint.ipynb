{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStatic():\n",
    "    '''a function to load static higher education provider data'''\n",
    "    \n",
    "    df = pd.read_csv(\"Provider.csv\").fillna(\n",
    "        value={'affiliation': 'NUHEP', 'affil_status': 'current'})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadEftsl():\n",
    "    '''a function to load equivalent full-time student load (eftsl)'''\n",
    "    \n",
    "    df = pd.read_csv('19_381_Onshore_EFTSL.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadPerformance():\n",
    "    \n",
    "    '''a function to load, clean and join student success rate and attrition rate data'''\n",
    "    \n",
    "    #load success rate data\n",
    "    success_all = pd.read_csv(\"19_381_Onshore_success_rate_overall.csv\").rename(\n",
    "        columns = {\"success_rate\": \"success_all\"}).drop(['provider_name'], axis=1)\n",
    "    \n",
    "    success_dom = pd.read_csv(\"19_381_Onshore_success_domestic.csv\").rename(\n",
    "        columns = {\"success_rate\": \"success_dom\"})[['code_year', 'success_dom']]\n",
    "    \n",
    "    success_int = pd.read_csv(\"19_381_Onshore_success_international.csv\").rename(\n",
    "        columns = {\"success_rate\": \"success_int\"})[['code_year', 'success_int']]\n",
    "    \n",
    "    # load attrition rate data\n",
    "    attrition_all = pd.read_csv(\"19_381_Onshore_attrition_rate_overall.csv\").rename(\n",
    "        columns = {'attrition_rate': 'attrition_all'})[['code_year', 'attrition_all']]\n",
    "    \n",
    "    attrition_dom = pd.read_csv(\"19_381_Onshore_attrition_domestic.csv\").rename(\n",
    "        columns = {'attrition_rate': 'attrition_dom'})[['code_year', 'attrition_dom']]\n",
    "    \n",
    "    attrition_int = pd.read_csv(\"19_381_Onshore_attrition_international.csv\").rename(\n",
    "        columns = {'attrition_rate': 'attrition_int'})[['code_year', 'attrition_int']]\n",
    "    \n",
    "    # merge data frames\n",
    "    df = success_all.merge(success_dom, on='code_year', how = 'outer').merge(\n",
    "        success_int, on='code_year', how = 'outer').merge(\n",
    "        attrition_all, on='code_year', how = 'outer').merge(\n",
    "        attrition_dom, on='code_year', how = 'outer').merge(\n",
    "        attrition_int, on='code_year', how = 'outer')\n",
    "    \n",
    "    return df[df['ref_year'] > 2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStaff(eftsl):\n",
    "\n",
    "    '''a function to load staff data and create important staff-related features'''\n",
    "    \n",
    "    # load .csv\n",
    "    staff = pd.read_csv('19_381_Academic_staff.csv').rename(\n",
    "        columns = {'tab1_academic_FTE': 'all_fte', \n",
    "                   'tab2_academic_FTE': 'salaried_fte', \n",
    "                   'tab3_senior_FTE': 'senior_fte', \n",
    "                   'tab3_senior_headcount_mod': 'senior_headcount'})\n",
    "    \n",
    "    # calculate pure staffing features\n",
    "    staff['sessional_fte'] = staff['all_fte'] - staff['salaried_fte']\n",
    "    staff['sessional_prop'] = staff['sessional_fte'] / staff['all_fte']\n",
    "    staff['senior_prop'] = staff['senior_fte'] / staff['all_fte']\n",
    "    \n",
    "    # join with eftsl data to enable calculation of student:staff ratio\n",
    "    eftsl_total = eftsl[['code_year', 'EFTSL']].groupby(['code_year']).sum().reset_index()\n",
    "    staff = staff.merge(eftsl_total, on='code_year').rename(columns={'EFTSL': 'eftsl'})\n",
    "    \n",
    "    # calculate student:staff ratios\n",
    "    staff['ssr_all'] = staff['eftsl'] / staff['all_fte']\n",
    "    staff['ssr_salaried'] = staff['eftsl'] / staff['salaried_fte']\n",
    "    \n",
    "    # select staffing features\n",
    "    df = staff[['code_year', \n",
    "                'all_fte', \n",
    "                'salaried_fte', \n",
    "                'senior_fte', \n",
    "                'senior_headcount', \n",
    "                'sessional_fte', \n",
    "                'sessional_prop', \n",
    "                'senior_prop', \n",
    "                'ssr_all']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildPostgrad(eftsl):\n",
    "    \n",
    "    ''' a function to extract postgraduate student proportions from eftsl data'''\n",
    "    \n",
    "    # aggregate eftsl at course level by provider by year\n",
    "    df = eftsl.drop(['provider_code', 'ref_year'], axis=1).groupby(['Course_level','code_year']).sum().reset_index()\n",
    "    \n",
    "    # pivot in order to make columnwise calculation\n",
    "    df = df.pivot(index='code_year', columns='Course_level', values='EFTSL').fillna(value=0)\n",
    "    \n",
    "    # flatten index after pivoting\n",
    "    df.columns = [''.join(col).strip() for col in df.columns.values]\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    # calculate postgraduate eftsl proportion and drop unnecessary columns\n",
    "    df['postgrad_prop'] = df['Postgrad by course'] / (df['Postgrad by course'] + df['Undergrad'])\n",
    "    df = df.drop(['Postgrad by course', 'Undergrad'], axis=1).fillna(value=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildInternational(eftsl):\n",
    "    \n",
    "    ''' a function to extract international student proportions from eftsl data'''\n",
    "    \n",
    "    # aggregate eftsl at citizenship type (domestic or international) by provider by year\n",
    "    df = eftsl.drop(['provider_code', 'ref_year'], axis=1).groupby(['citizenship','code_year']).sum().reset_index()\n",
    "    \n",
    "    # pivot in order to make columnwise calculation\n",
    "    df = df.pivot(index='code_year', columns='citizenship', values='EFTSL').fillna(value=0)\n",
    "    \n",
    "    # flatten index after pivoting\n",
    "    df.columns = [''.join(col).strip() for col in df.columns.values]\n",
    "    df.reset_index(inplace = True)\n",
    "    \n",
    "    # calculate international eftsl proportion and drop unnecessary columns\n",
    "    df['int_prop'] = df['International'] / (df['International'] + df['Domestic'])\n",
    "    df = df.drop(['International', 'Domestic'], axis=1).fillna(value=0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildBfoe(eftsl):\n",
    "    \n",
    "    '''a function to generate eftsl and bfoe-related features'''\n",
    "\n",
    "    # aggregate eftsl at BFOE type by provider by year\n",
    "    df = eftsl.drop(['provider_code', 'ref_year'], axis=1).groupby(['primary_BFOE','code_year']).sum().reset_index()\n",
    "\n",
    "    # pivot wider and replace NAs with zeros to ensure each provider in each year has a value for each BFOE\n",
    "    df = df.pivot(index='code_year', columns='primary_BFOE', values='EFTSL').fillna(value=0)\n",
    "\n",
    "    # rename BFOE categories to be shorter\n",
    "    df = df.rename(columns = {'01 Natural and Physical Sciences': 'nat_phys_sci',\n",
    "                             '02 Information Technology': 'info_tech',\n",
    "                             '03 Engineering and Related Technologies': 'engineering',\n",
    "                             '04 Architecture and Building': 'arch_build',\n",
    "                             '05 Agriculture, Environmental and Related Studies': 'agri_env',\n",
    "                             '06 Health': 'health',\n",
    "                             '07 Education': 'education',\n",
    "                             '08 Management and Commerce': 'mge_com',\n",
    "                             '09 Society and Culture': 'soc_cult',\n",
    "                             '10 Creative Arts': 'creat_art',\n",
    "                             '11 Food, Hospitality and Personal Services': 'food_hosp',\n",
    "                             '12 Mixed Field Programmes': 'mixed',\n",
    "                             '13 Non-award courses': 'non_award'})\n",
    "\n",
    "    vars = ['nat_phys_sci', \n",
    "            'info_tech', \n",
    "            'engineering', \n",
    "            'arch_build', \n",
    "            'agri_env', \n",
    "            'health', \n",
    "            'education', \n",
    "            'mge_com', \n",
    "            'soc_cult', \n",
    "            'creat_art', \n",
    "            'food_hosp',\n",
    "            'mixed',\n",
    "            'non_award']\n",
    "\n",
    "    # flatten index after pivoting\n",
    "    df.columns = [''.join(col).strip() for col in df.columns.values]\n",
    "    df.reset_index(inplace = True)\n",
    "\n",
    "    # return to long format to implement column-wise calculations\n",
    "    df = pd.melt(df, id_vars=['code_year'], value_vars=vars, var_name='bfoe', value_name='eftsl')\n",
    "\n",
    "    # find max bfoe eftsl for each provider\n",
    "    df['bfoe_max'] = df.groupby(['code_year'])['eftsl'].transform(max)\n",
    "    df['bfoe_max'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # caclulate total eftsl for ech provider\n",
    "    df['eftsl_sum'] = df.groupby(['code_year'])['eftsl'].transform(sum)\n",
    "\n",
    "    # calculate eftsl proportion for each bfoe for each provider for each year\n",
    "    df['eftsl_prop'] = df['eftsl'] / df['eftsl_sum']\n",
    "    df['eftsl_prop'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "    # calculate values to be summed for entropy calculation\n",
    "    df['pre_entropy'] = -1*df['eftsl_prop']*np.log2(df['eftsl_prop'])\n",
    "\n",
    "    #calculate entropy\n",
    "    df['bfoe_entropy'] = df.groupby(['code_year'])['pre_entropy'].transform(sum)\n",
    "\n",
    "    # calculate values to be summed for gini impurity calculation\n",
    "    df['pre_gini'] = df['eftsl_prop']*(1-df['eftsl_prop'])\n",
    "\n",
    "    # calculate gini impurity\n",
    "    df['bfoe_gini_impurity'] = df.groupby(['code_year'])['pre_gini'].transform(sum)\n",
    "\n",
    "    #filter to only the max bfoe for any given year\n",
    "    df = df[df['bfoe_max'] == df['eftsl']] \n",
    "    df = df[df['bfoe_max'] != 0]\n",
    "\n",
    "    # drop unnecessary columns and rename as required\n",
    "    df = df.drop(['eftsl', 'bfoe_max', 'pre_entropy', 'pre_gini'], axis=1).rename(\n",
    "        columns={'bfoe': 'primary_bfoe', 'eftsl_prop': 'primary_bfoe_prop'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAverages():\n",
    "    \n",
    "    '''a function to load subsector averages published by TEQSA'''\n",
    "    \n",
    "    TEQSA_avg = pd.read_csv('TEQSA_averages.csv')\n",
    "    \n",
    "    return TEQSA_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDecisions(max_year = 2017):\n",
    "    decisions = pd.read_csv(\"https://data.gov.au/data/dataset/0c4f6591-2aea-4797-a127-ae8f8a0be0e2/resource/a61abca4-a4b7-4f17-a975-0bba20c2c73f/download/outcomes_26102020.csv\")\n",
    "    decisions = decisions.drop('Date',axis=1)\n",
    "    \n",
    "    providers = pd.read_csv('https://data.gov.au/data/dataset/0c4f6591-2aea-4797-a127-ae8f8a0be0e2/resource/07370e3f-780b-4a70-8c87-b6796d5ab237/download/providers_21102020.csv')\n",
    "    providers = providers[['ProviderID', 'ProviderName', 'Category']]\n",
    "\n",
    "    decisions['Description'] = decisions.Title.str.rsplit(n=3).str[0]\n",
    "    decisions['Year'] = decisions.Title.str.rsplit(n=1).str[-1]\n",
    "\n",
    "    decisions = decisions.drop('Title',axis=1)\n",
    "\n",
    "    decisions['dec_condition'] = decisions['Text'].str.contains(\n",
    "        'ondition')|decisions['Description'].str.contains(\n",
    "        'ondition')\n",
    "    decisions['dec_staff_ratio'] = decisions['Text'].str.contains('staff ratio|SSR')\n",
    "    decisions['dec_sessional'] = decisions['Text'].str.contains('essional|asual')\n",
    "    decisions['dec_attrition'] = decisions['Text'].str.contains('ttrition')\n",
    "    decisions['dec_progress'] = decisions['Text'].str.contains('uccess|rogress')\n",
    "    decisions['dec_rejected'] = (decisions['Description'].str.contains('reject|Reject|not|Not|Cancel r|cancel r|Cancel a|cancel a')|\n",
    "                                 decisions['CourseID'].str.contains('reject|Reject'))\n",
    "    decisions['extension'] = decisions['Description'].str.contains('Exten|exten')\n",
    "    \n",
    "    decisions['dec_any_adverse'] = (decisions['dec_rejected']|\n",
    "                decisions['dec_condition']|\n",
    "                decisions['dec_staff_ratio']|\n",
    "                decisions['dec_sessional']|\n",
    "                decisions['dec_attrition']|\n",
    "                decisions['dec_progress'])\n",
    "\n",
    "    decisions['dec_formal_adverse'] = (decisions['dec_rejected']|\n",
    "                    decisions['dec_condition'])\n",
    "    \n",
    "    decisions = decisions[decisions['extension'] == False]\n",
    "\n",
    "    decisions = decisions.drop('Text',axis=1)\n",
    "    decisions['ProviderID'] = decisions['ProviderID'].str.strip()\n",
    "\n",
    "    decisions = providers.merge(decisions, on = 'ProviderID', how = 'outer')\n",
    "\n",
    "    decisions = decisions[decisions['Type'] == 'Decision']\n",
    "    decisions = decisions.drop('Type',axis=1)\n",
    "    decisions['DecisionType'] = decisions['DecisionType'].str.strip()\n",
    "\n",
    "    decisions = decisions[decisions.DecisionType.isin(['Registration',\n",
    "                                                       'Re-registration',\n",
    "                                                       'Re-accreditation', \n",
    "                                                       'Accreditation'])]\n",
    "\n",
    "    decisions = decisions[[ 'ProviderID',\n",
    "                            'Year', \n",
    "                            'DecisionType',\n",
    "                            'dec_any_adverse',\n",
    "                            'dec_formal_adverse',\n",
    "                            'dec_rejected',\n",
    "                            'dec_condition',\n",
    "                            'dec_staff_ratio',\n",
    "                            'dec_sessional',\n",
    "                            'dec_attrition',\n",
    "                            'dec_progress']]\n",
    "    decisions.rename(columns={\"ProviderID\" : \"prv\", \"Year\" : \"ref_year\"}, inplace=True)\n",
    "    \n",
    "    decisions.ref_year = pd.to_numeric(decisions.ref_year, errors='coerce').astype('Int64')\n",
    "\n",
    "    #decisions[decisions['ref_year'] > max_year] = max_year\n",
    "    \n",
    "    decisions = decisions.groupby(['prv', 'ref_year', 'DecisionType']).sum().astype(int).reset_index()\n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinData():\n",
    "    '''a function to join all higher ed data frames''' \n",
    "\n",
    "    # load datasets\n",
    "    eftsl = loadEftsl()\n",
    "    provider = loadStatic()\n",
    "    performance = loadPerformance()\n",
    "    staff = loadStaff(eftsl)\n",
    "    teqsa_avg = loadAverages()\n",
    "    \n",
    "    # transform eftsl data\n",
    "    bfoe = buildBfoe(eftsl)\n",
    "    postgrad = buildPostgrad(eftsl)\n",
    "    international = buildInternational(eftsl)\n",
    "\n",
    "    # join datasets\n",
    "    df = provider.merge(performance, on='provider_code', how = 'outer').merge(\n",
    "        staff, on='code_year', how = 'outer').merge(\n",
    "        bfoe, on='code_year', how = 'outer').merge(\n",
    "        postgrad, on='code_year', how = 'outer').merge(\n",
    "        international, on='code_year', how = 'outer').merge(\n",
    "        teqsa_avg, on=['TEQSA_type', 'ref_year'], how = 'outer')\n",
    "    # remove providers without a name\n",
    "    df = df[df.provider_name.notnull()]\n",
    "\n",
    "    # remove ref_year before 2012\n",
    "    df = df[df['ref_year'] > 2011]\n",
    "\n",
    "    df.ref_year = df.ref_year.astype(int)\n",
    "\n",
    "    # remove providers that no longer existed in the last year of the data collection (2017)\n",
    "    #still_exists = list(df[df['ref_year'] == 2017]['prv'])\n",
    "    #df = df[df.prv.isin(still_exists)]\n",
    "\n",
    "    # replace unlikely values with null\n",
    "    df['ssr_all'] = np.where((df.ssr_all < 0.00001), np.NaN ,df.ssr_all)\n",
    "    df['sessional_prop'] = np.where((df.sessional_prop < 0.00001), np.NaN ,df.sessional_prop)\n",
    "    df['sessional_prop'] = np.where((df.sessional_prop > 0.99999), np.NaN ,df.sessional_prop)\n",
    "\n",
    "\n",
    "    # remove redundant provider code\n",
    "    df = df.drop(['provider_code'], axis=1)\n",
    "    \n",
    "\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(df):\n",
    "    '''a function to impute missing values using MICE'''\n",
    "    \n",
    "    # select numeric features\n",
    "    df2 = df[['primary_bfoe_prop',\n",
    "                'bfoe_entropy',\n",
    "                'bfoe_gini_impurity',\n",
    "                'postgrad_prop',\n",
    "                'int_prop',\n",
    "                'sessional_prop_type_avg',\n",
    "                'sessional_prop',\n",
    "                'senior_prop',\n",
    "                'ssr_all_type_avg',\n",
    "                'ssr_salaried_type_avg',\n",
    "                'ssr_all',\n",
    "                'success_dom',\n",
    "                'success_int',\n",
    "                'success_all',\n",
    "                'attrition_dom',\n",
    "                'attrition_int',\n",
    "                'attrition_all']]\n",
    "\n",
    "    # specify minimum and maximum values for imputation\n",
    "    min = [0.01, 0, 0, 0, 0, 0.01, 0.01, 0, 0, 0, 0.01, 1, 1, 1, 1, 1, 1]\n",
    "    max = [1, 3.05, 1, 1, 1, 1, 1, 1, 200, 200, 200, 100, 100, 100, 100, 100, 100]\n",
    "    \n",
    "    # instantiate imputer\n",
    "    mice_imputer = IterativeImputer(max_iter=10000, random_state=1, min_value=min, max_value=max)\n",
    "    \n",
    "    # fit imputation and return as a dataframe\n",
    "    df_imp = pd.DataFrame(mice_imputer.fit_transform(df2).round(decimals=6))\n",
    "    \n",
    "    # rename imputed columns\n",
    "    df_imp.columns = ['primary_bfoe_prop',\n",
    "                'bfoe_entropy',\n",
    "                'bfoe_gini_impurity',\n",
    "                'postgrad_prop',\n",
    "                'int_prop',\n",
    "                'sessional_prop_type_avg',\n",
    "                'sessional_prop',\n",
    "                'senior_prop',\n",
    "                'ssr_all_type_avg',\n",
    "                'ssr_salaried_type_avg',\n",
    "                'ssr_all',\n",
    "                'success_dom',\n",
    "                'success_int',\n",
    "                'success_all',\n",
    "                'attrition_dom',\n",
    "                'attrition_int',\n",
    "                'attrition_all']\n",
    "    \n",
    "    # specify columns to be overwritten with imputed data, noting that cells without NAs will be untouched\n",
    "    columns_to_overwrite = ['primary_bfoe_prop',\n",
    "                'bfoe_entropy',\n",
    "                'bfoe_gini_impurity',\n",
    "                'postgrad_prop',\n",
    "                'int_prop',\n",
    "                'sessional_prop_type_avg',\n",
    "                'sessional_prop',\n",
    "                'senior_prop',\n",
    "                'ssr_all_type_avg',\n",
    "                'ssr_salaried_type_avg',\n",
    "                'ssr_all',\n",
    "                'success_dom',\n",
    "                'success_int',\n",
    "                'success_all',\n",
    "                'attrition_dom',\n",
    "                'attrition_int',\n",
    "                'attrition_all']\n",
    "    \n",
    "    # drop columns to be overwritten from original dataset and replace with imputed data\n",
    "    df3 = df.drop(columns_to_overwrite, axis=1)\n",
    "    df3[columns_to_overwrite] = df_imp[columns_to_overwrite]\n",
    "\n",
    "    return df3[df3['ssr_all'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staffAdjust(df_imp, sessional_multiplier = 1, ssr_multiplier = 1):\n",
    "    \n",
    "    df_imp = df_imp\n",
    "    \n",
    "    '''a function to adjust imputed staffing values based on sub-sector averages published by TEQSA'''\n",
    "\n",
    "    compare = df_imp[['ref_year','TEQSA_type',\n",
    "                    'sessional_prop_type_avg',\n",
    "                    'sessional_prop',\n",
    "                    'ssr_all_type_avg',\n",
    "                    'ssr_salaried_type_avg',\n",
    "                    'ssr_all']]\n",
    "\n",
    "    agg = compare.groupby(['ref_year','TEQSA_type']).mean().reset_index()\n",
    "    agg['sessional_diff'] = agg['sessional_prop_type_avg'] - agg['sessional_prop']\n",
    "    agg['ssr_all_diff'] = agg['ssr_all_type_avg'] - agg['ssr_all']\n",
    "    diff = agg.drop(['sessional_prop_type_avg', 'sessional_prop', 'ssr_all_type_avg', 'ssr_all', 'ssr_salaried_type_avg'], axis=1)\n",
    "\n",
    "    df_diff = df_imp.merge(diff, on=['ref_year', 'TEQSA_type'], how = 'left')\n",
    "\n",
    "    df_diff.loc[df_diff.TEQSA_type!='university', 'sessional_prop'] = sessional_multiplier * (df_diff.loc[df_diff.TEQSA_type!='university', 'sessional_prop'] + df_diff.loc[df_diff.TEQSA_type!='university', 'sessional_diff'])\n",
    "    df_diff.loc[df_diff.TEQSA_type!='university', 'ssr_all'] = ssr_multiplier * (df_diff.loc[df_diff.TEQSA_type!='university', 'ssr_all'] + df_diff.loc[df_diff.TEQSA_type!='university', 'ssr_all_diff'])\n",
    "\n",
    "    df_diff = df_diff.drop(['sessional_diff', 'ssr_all_diff', 'sessional_prop_type_avg', 'ssr_all_type_avg', 'ssr_salaried_type_avg'], axis=1)\n",
    "\n",
    "    df_diff['ssr_salaried'] = df_diff['ssr_all'] * (1 / (1 - df_diff['sessional_prop']))\n",
    "    \n",
    "    df = df_diff[['code_year',\n",
    "            'prv',\n",
    "            'provider_name',\n",
    "            'ref_year',\n",
    "            'type',\n",
    "            'TEQSA_type',\n",
    "            'ownership',\n",
    "            'profit',\n",
    "            'affiliation',\n",
    "            'affil_status',\n",
    "            'eftsl_sum',\n",
    "            'primary_bfoe',\n",
    "            'primary_bfoe_prop',\n",
    "            'bfoe_entropy',\n",
    "            'bfoe_gini_impurity',\n",
    "            'postgrad_prop',\n",
    "            'int_prop',\n",
    "            'sessional_prop',\n",
    "            'senior_prop',\n",
    "            'ssr_all',\n",
    "            'ssr_salaried',\n",
    "            'success_dom',\n",
    "            'success_int',\n",
    "            'success_all',\n",
    "            'attrition_dom',\n",
    "            'attrition_int',\n",
    "            'attrition_all']]\n",
    "\n",
    "    return df.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    df_raw = joinData()\n",
    "    df_raw.to_csv('higher_ed_data_raw.csv', index = False)\n",
    "    \n",
    "    df_imp = impute(df_raw)\n",
    "    df_imp = staffAdjust(df_imp)\n",
    "    df_imp.to_csv('higher_ed_data_imputed.csv', index = False)\n",
    "    \n",
    "    df_dec = loadDecisions(max_year = 2021)\n",
    "    df_dec.to_csv('teqsa_decisions.csv', index = False)\n",
    "    \n",
    "    return df_raw, df_imp, df_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw, df_imp, df_dec = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
